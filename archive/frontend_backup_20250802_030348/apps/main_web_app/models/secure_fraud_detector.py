#!/usr/bin/env python3
"""
ì—„ê²©í•œ ë°ì´í„° ë¶„ë¦¬ ë° ì˜¤ë²„í”¼íŒ… ë°©ì§€ ì‚¬ê¸°íƒì§€ ëª¨ë¸
===================================================

ì™„ì „í•œ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ì™€ ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ
ì—„ê²©í•œ ì‚¬ê¸°íƒì§€ ëª¨ë¸ íŒŒì´í”„ë¼ì¸
"""

import pandas as pd
import numpy as np
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import os
import joblib
import json

# ML ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import TimeSeriesSplit, cross_validate, validation_curve
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, average_precision_score, confusion_matrix,
    classification_report, roc_curve, precision_recall_curve
)
from sklearn.utils.class_weight import compute_class_weight
# from imblearn.pipeline import Pipeline as ImbPipeline
# from imblearn.over_sampling import SMOTE, ADASYN
# from imblearn.under_sampling import RandomUnderSampler
# from imblearn.combine import SMOTETomek

warnings.filterwarnings('ignore')


class TemporalSplitter:
    """ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• ê¸°"""
    
    def __init__(self, time_column: str = 'Time'):
        self.time_column = time_column
        self.split_info = {}
    
    def temporal_train_test_split(self, df: pd.DataFrame, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ì‹œê°„ ìˆœì„œë¥¼ ê³ ë ¤í•œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• """
        
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ í™•ì¸
        if not df[self.time_column].is_monotonic_increasing:
            print("âš ï¸ ë°ì´í„°ê°€ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì§€ ì•ŠìŒ. ì •ë ¬ ì¤‘...")
            df = df.sort_values(self.time_column).reset_index(drop=True)
        
        # ì‹œê°„ ê¸°ë°˜ ë¶„í• ì  ê³„ì‚°
        split_idx = int(len(df) * (1 - test_size))
        
        train_df = df.iloc[:split_idx].copy()
        test_df = df.iloc[split_idx:].copy()
        
        # ë¶„í•  ì •ë³´ ì €ì¥
        self.split_info = {
            'train_start_time': train_df[self.time_column].min(),
            'train_end_time': train_df[self.time_column].max(),
            'test_start_time': test_df[self.time_column].min(),
            'test_end_time': test_df[self.time_column].max(),
            'train_size': len(train_df),
            'test_size': len(test_df),
            'temporal_gap': test_df[self.time_column].min() - train_df[self.time_column].max()
        }
        
        print(f"âœ… ì‹œê°„ ê¸°ë°˜ ë¶„í•  ì™„ë£Œ:")
        print(f"  í›ˆë ¨ ì„¸íŠ¸: {len(train_df):,}ê°œ (ì‹œê°„: {self.split_info['train_start_time']:.1f} ~ {self.split_info['train_end_time']:.1f})")
        print(f"  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(test_df):,}ê°œ (ì‹œê°„: {self.split_info['test_start_time']:.1f} ~ {self.split_info['test_end_time']:.1f})")
        print(f"  ì‹œê°„ ê°„ê²©: {self.split_info['temporal_gap']:.1f}")
        
        return train_df, test_df


class DataLeakageValidator:
    """ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.validation_results = {}
        self.critical_issues = []
        self.warnings = []
    
    def validate_temporal_integrity(self, train_df: pd.DataFrame, test_df: pd.DataFrame, time_column: str = 'Time') -> Dict[str, Any]:
        """ì‹œê°„ì  ë¬´ê²°ì„± ê²€ì¦"""
        
        validation = {
            'temporal_overlap': False,
            'future_leakage': False,
            'time_gap_adequate': False,
            'severity': 'LOW'
        }
        
        train_max_time = train_df[time_column].max()
        test_min_time = test_df[time_column].min()
        
        # 1. ì‹œê°„ì  ì¤‘ë³µ í™•ì¸
        if train_max_time >= test_min_time:
            validation['temporal_overlap'] = True
            self.critical_issues.append(f"ì‹œê°„ì  ì¤‘ë³µ: í›ˆë ¨ ìµœëŒ€ì‹œê°„({train_max_time}) >= í…ŒìŠ¤íŠ¸ ìµœì†Œì‹œê°„({test_min_time})")
            validation['severity'] = 'CRITICAL'
        
        # 2. ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ í™•ì¸
        if (test_df[time_column] < train_df[time_column].max()).any():
            validation['future_leakage'] = True
            self.critical_issues.append("ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ: ì¼ë¶€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ í›ˆë ¨ ê¸°ê°„ì— í¬í•¨ë¨")
            validation['severity'] = 'CRITICAL'
        
        # 3. ì¶©ë¶„í•œ ì‹œê°„ ê°„ê²© í™•ì¸
        time_gap = test_min_time - train_max_time
        if time_gap > 0:
            validation['time_gap_adequate'] = True
            print(f"âœ… ì ì ˆí•œ ì‹œê°„ ê°„ê²©: {time_gap:.1f}")
        else:
            self.warnings.append(f"ì‹œê°„ ê°„ê²© ë¶€ì¡±: {time_gap:.1f}")
        
        return validation
    
    def validate_feature_integrity(self, train_df: pd.DataFrame, test_df: pd.DataFrame, target_col: str = 'Class') -> Dict[str, Any]:
        """íŠ¹ì„± ë¬´ê²°ì„± ê²€ì¦"""
        
        validation = {
            'statistical_leakage': [],
            'distribution_shift': [],
            'severity': 'LOW'
        }
        
        train_X = train_df.drop(target_col, axis=1)
        test_X = test_df.drop(target_col, axis=1)
        
        # í†µê³„ì  ëˆ„ì¶œ í™•ì¸ (í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ê°„ ë¹„ì •ìƒì  ìœ ì‚¬ì„±)
        for col in train_X.select_dtypes(include=[np.number]).columns:
            if col == 'Time':  # Time ì»¬ëŸ¼ì€ ì œì™¸
                continue
                
            train_mean = train_X[col].mean()
            test_mean = test_X[col].mean()
            train_std = train_X[col].std()
            test_std = test_X[col].std()
            
            # í‰ê·  ì°¨ì´ í™•ì¸
            if train_std > 0 and test_std > 0:
                mean_diff = abs(train_mean - test_mean) / train_std
                std_ratio = max(train_std, test_std) / min(train_std, test_std)
                
                if mean_diff > 3.0:  # 3 í‘œì¤€í¸ì°¨ ì´ìƒ ì°¨ì´
                    validation['distribution_shift'].append({
                        'feature': col,
                        'mean_diff_std': mean_diff,
                        'severity': 'HIGH'
                    })
                    self.warnings.append(f"ë¶„í¬ ë³€í™” ê°ì§€: {col} (í‰ê·  ì°¨ì´: {mean_diff:.2f}Ïƒ)")
                
                if std_ratio > 3.0:  # í‘œì¤€í¸ì°¨ 3ë°° ì´ìƒ ì°¨ì´
                    validation['distribution_shift'].append({
                        'feature': col,
                        'std_ratio': std_ratio,
                        'severity': 'HIGH'
                    })
                    self.warnings.append(f"ë¶„ì‚° ë³€í™” ê°ì§€: {col} (í‘œì¤€í¸ì°¨ ë¹„ìœ¨: {std_ratio:.2f})")
        
        return validation


class SecurePreprocessor(BaseEstimator, TransformerMixin):
    """ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, handle_outliers: bool = True, feature_selection: bool = True):
        self.handle_outliers = handle_outliers
        self.feature_selection = feature_selection
        self.scaler = None
        self.feature_selector = None
        self.outlier_detector = None
        self.feature_names = None
        self.preprocessing_stats = {}
    
    def fit(self, X, y=None):
        """í›ˆë ¨ ë°ì´í„°ì—ë§Œ í”¼íŒ… (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)"""
        
        print("ğŸ”§ ì „ì²˜ë¦¬ê¸° í›ˆë ¨ ì¤‘...")
        
        # íŠ¹ì„±ëª… ì €ì¥
        if hasattr(X, 'columns'):
            self.feature_names = X.columns.tolist()
            X_array = X.values
        else:
            X_array = X
            self.feature_names = [f'feature_{i}' for i in range(X_array.shape[1])]
        
        # 1. ìŠ¤ì¼€ì¼ë§ (RobustScaler - ì´ìƒì¹˜ì— ëœ ë¯¼ê°)
        self.scaler = RobustScaler()
        X_scaled = self.scaler.fit_transform(X_array)
        
        # 2. ì´ìƒì¹˜ íƒì§€ ë° ì œê±° (ì˜µì…˜)
        if self.handle_outliers:
            self.outlier_detector = IsolationForest(
                contamination=0.1,  # 10% ì´ìƒì¹˜ ê°€ì •
                random_state=42,
                n_jobs=-1
            )
            outlier_labels = self.outlier_detector.fit_predict(X_scaled)
            outlier_mask = outlier_labels == 1  # ì •ìƒ ë°ì´í„°
            
            self.preprocessing_stats['outliers_detected'] = np.sum(outlier_labels == -1)
            self.preprocessing_stats['outlier_ratio'] = self.preprocessing_stats['outliers_detected'] / len(X_array)
            
            print(f"  ì´ìƒì¹˜ íƒì§€: {self.preprocessing_stats['outliers_detected']:,}ê°œ ({self.preprocessing_stats['outlier_ratio']*100:.2f}%)")
        
        # 3. íŠ¹ì„± ì„ íƒ (ì˜µì…˜)
        if self.feature_selection and y is not None:
            # íƒ€ê²Ÿì´ ìˆëŠ” ê²½ìš°ì—ë§Œ íŠ¹ì„± ì„ íƒ
            if self.handle_outliers:
                X_for_selection = X_scaled[outlier_mask]
                y_for_selection = y[outlier_mask] if hasattr(y, '__len__') else y
            else:
                X_for_selection = X_scaled
                y_for_selection = y
            
            # Random Forest ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ì„ íƒ (ê°€ë²¼ìš´ ë²„ì „)
            rf_selector = RandomForestClassifier(n_estimators=20, random_state=42, n_jobs=-1)
            self.feature_selector = SelectFromModel(rf_selector, threshold='median')
            self.feature_selector.fit(X_for_selection, y_for_selection)
            
            selected_features = self.feature_selector.get_support()
            self.preprocessing_stats['features_selected'] = np.sum(selected_features)
            self.preprocessing_stats['features_removed'] = len(selected_features) - np.sum(selected_features)
            
            print(f"  íŠ¹ì„± ì„ íƒ: {self.preprocessing_stats['features_selected']}ê°œ ì„ íƒ, {self.preprocessing_stats['features_removed']}ê°œ ì œê±°")
        
        return self
    
    def transform(self, X):
        """ë°ì´í„° ë³€í™˜ (í›ˆë ¨ëœ ì „ì²˜ë¦¬ê¸° ì ìš©)"""
        
        if hasattr(X, 'columns'):
            X_array = X.values
        else:
            X_array = X
        
        # 1. ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scaler.transform(X_array)
        
        # 2. íŠ¹ì„± ì„ íƒ ì ìš©
        if self.feature_selection and self.feature_selector is not None:
            X_scaled = self.feature_selector.transform(X_scaled)
        
        return X_scaled
    
    def get_feature_names_out(self):
        """ì„ íƒëœ íŠ¹ì„±ëª… ë°˜í™˜"""
        if self.feature_selection and self.feature_selector is not None:
            mask = self.feature_selector.get_support()
            return [name for name, selected in zip(self.feature_names, mask) if selected]
        return self.feature_names


class OverfittingPreventer:
    """ì˜¤ë²„í”¼íŒ… ë°©ì§€ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.validation_curves = {}
        self.learning_curves = {}
        self.cross_val_results = {}
    
    def analyze_model_complexity(self, pipeline, X, y, param_name, param_range, cv=5):
        """ëª¨ë¸ ë³µì¡ë„ ë¶„ì„"""
        
        print(f"ğŸ“ˆ ëª¨ë¸ ë³µì¡ë„ ë¶„ì„ ì¤‘: {param_name}")
        
        train_scores, validation_scores = validation_curve(
            pipeline, X, y,
            param_name=param_name,
            param_range=param_range,
            cv=cv,
            scoring='f1',
            n_jobs=-1
        )
        
        self.validation_curves[param_name] = {
            'param_range': param_range,
            'train_scores_mean': np.mean(train_scores, axis=1),
            'train_scores_std': np.std(train_scores, axis=1),
            'validation_scores_mean': np.mean(validation_scores, axis=1),
            'validation_scores_std': np.std(validation_scores, axis=1)
        }
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸° (ê²€ì¦ ì ìˆ˜ê°€ ìµœëŒ€ì¸ ì§€ì )
        optimal_idx = np.argmax(self.validation_curves[param_name]['validation_scores_mean'])
        optimal_param = param_range[optimal_idx]
        
        # ì˜¤ë²„í”¼íŒ… ê°ì§€
        train_val_gap = (
            self.validation_curves[param_name]['train_scores_mean'][optimal_idx] - 
            self.validation_curves[param_name]['validation_scores_mean'][optimal_idx]
        )
        
        overfitting_severity = 'LOW'
        if train_val_gap > 0.1:
            overfitting_severity = 'HIGH'
        elif train_val_gap > 0.05:
            overfitting_severity = 'MEDIUM'
        
        print(f"  ìµœì  {param_name}: {optimal_param}")
        print(f"  ì˜¤ë²„í”¼íŒ… ì •ë„: {overfitting_severity} (ê°­: {train_val_gap:.3f})")
        
        return optimal_param, overfitting_severity
    
    def perform_robust_cross_validation(self, pipeline, X, y, cv_method='timeseries'):
        """ê°•ê±´í•œ êµì°¨ ê²€ì¦"""
        
        print(f"ğŸ”„ {cv_method} êµì°¨ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
        
        if cv_method == 'timeseries':
            cv = TimeSeriesSplit(n_splits=3)  # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì¤„ì„
        else:
            from sklearn.model_selection import StratifiedKFold
            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # ë¹ ë¥¸ ì‹¤í–‰
        
        scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
        
        cv_results = cross_validate(
            pipeline, X, y,
            cv=cv,
            scoring=scoring,
            return_train_score=True,
            n_jobs=-1
        )
        
        # ê²°ê³¼ ì •ë¦¬
        self.cross_val_results = {}
        for metric in scoring:
            test_scores = cv_results[f'test_{metric}']
            train_scores = cv_results[f'train_{metric}']
            
            self.cross_val_results[metric] = {
                'test_mean': np.mean(test_scores),
                'test_std': np.std(test_scores),
                'train_mean': np.mean(train_scores),
                'train_std': np.std(train_scores),
                'overfitting_gap': np.mean(train_scores) - np.mean(test_scores),
                'stability': np.std(test_scores) < 0.05  # 5% ë¯¸ë§Œ ë³€ë™ì„±
            }
        
        # ê²°ê³¼ ì¶œë ¥
        print("ğŸ“Š êµì°¨ ê²€ì¦ ê²°ê³¼:")
        for metric, results in self.cross_val_results.items():
            print(f"  {metric.upper()}:")
            print(f"    í…ŒìŠ¤íŠ¸: {results['test_mean']:.3f} Â± {results['test_std']:.3f}")
            print(f"    í›ˆë ¨: {results['train_mean']:.3f} Â± {results['train_std']:.3f}")
            print(f"    ì˜¤ë²„í”¼íŒ… ê°­: {results['overfitting_gap']:.3f}")
            print(f"    ì•ˆì •ì„±: {'âœ…' if results['stability'] else 'âŒ'}")
        
        return self.cross_val_results


class SecureFraudDetector:
    """ì—„ê²©í•œ ì‚¬ê¸°íƒì§€ ëª¨ë¸"""
    
    def __init__(self, data_path: str, model_save_path: str = '/root/FCA/models/secure_fraud_model.pkl'):
        self.data_path = data_path
        self.model_save_path = model_save_path
        self.model_save_dir = os.path.dirname(model_save_path)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.temporal_splitter = TemporalSplitter()
        self.leakage_validator = DataLeakageValidator()
        self.preprocessor = SecurePreprocessor()
        self.overfitting_preventer = OverfittingPreventer()
        
        # ë°ì´í„° ë° ëª¨ë¸
        self.train_df = None
        self.test_df = None
        self.pipeline = None
        self.final_model = None
        
        # ê²°ê³¼ ì €ì¥
        self.validation_report = {}
        self.performance_metrics = {}
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.model_save_dir, exist_ok=True)
    
    def load_and_validate_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ê²€ì¦"""
        
        print("ğŸ” ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ ì¤‘...")
        
        df = pd.read_csv(self.data_path)
        
        # ê¸°ë³¸ ê²€ì¦
        print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(df):,}")
        print(f"  íŠ¹ì„± ìˆ˜: {len(df.columns) - 1}")
        print(f"  ì‚¬ê¸° ìƒ˜í”Œ: {sum(df['Class'] == 1):,} ({sum(df['Class'] == 1)/len(df)*100:.3f}%)")
        print(f"  ì •ìƒ ìƒ˜í”Œ: {sum(df['Class'] == 0):,} ({sum(df['Class'] == 0)/len(df)*100:.3f}%)")
        
        # ì‹œê°„ ìˆœì„œ í™•ì¸
        if 'Time' in df.columns:
            if df['Time'].is_monotonic_increasing:
                print("  âœ… ì‹œê°„ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì •ë ¬ë¨")
            else:
                print("  âš ï¸ ì‹œê°„ ë°ì´í„° ì •ë ¬ í•„ìš”")
                df = df.sort_values('Time').reset_index(drop=True)
        
        # ê²°ì¸¡ê°’ í™•ì¸
        missing_values = df.isnull().sum().sum()
        if missing_values > 0:
            print(f"  âš ï¸ ê²°ì¸¡ê°’ ë°œê²¬: {missing_values}ê°œ")
        else:
            print("  âœ… ê²°ì¸¡ê°’ ì—†ìŒ")
        
        return df
    
    def split_data_temporally(self, df: pd.DataFrame, test_size: float = 0.2):
        """ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• """
        
        print("â° ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í•  ì¤‘...")
        
        self.train_df, self.test_df = self.temporal_splitter.temporal_train_test_split(df, test_size)
        
        # ë°ì´í„° ëˆ„ì¶œ ê²€ì¦
        temporal_validation = self.leakage_validator.validate_temporal_integrity(
            self.train_df, self.test_df
        )
        
        feature_validation = self.leakage_validator.validate_feature_integrity(
            self.train_df, self.test_df
        )
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.validation_report['temporal_validation'] = temporal_validation
        self.validation_report['feature_validation'] = feature_validation
        self.validation_report['critical_issues'] = self.leakage_validator.critical_issues
        self.validation_report['warnings'] = self.leakage_validator.warnings
        
        # ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆë‹¤ë©´ ì¤‘ë‹¨
        if self.leakage_validator.critical_issues:
            raise ValueError(f"ì‹¬ê°í•œ ë°ì´í„° ëˆ„ì¶œ ë¬¸ì œ ë°œê²¬: {self.leakage_validator.critical_issues}")
        
        return self.train_df, self.test_df
    
    def build_secure_pipeline(self):
        """ë³´ì•ˆ ê°•í™”ëœ ML íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"""
        
        print("ğŸ”§ ë³´ì•ˆ ê°•í™”ëœ ML íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì¤‘...")
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
        # Note: SMOTE ë¯¸ì‚¬ìš©ìœ¼ë¡œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¡œ ëŒ€ì²´
        
        # ê¸°ë³¸ ë¶„ë¥˜ê¸°ë“¤
        classifiers = {
            'rf': RandomForestClassifier(
                n_estimators=50,  # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì¤„ì„
                max_depth=8,  # ê¹Šì´ ì œí•œìœ¼ë¡œ ì˜¤ë²„í”¼íŒ… ë°©ì§€
                min_samples_split=10,  # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                min_samples_leaf=5,   # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'lr': LogisticRegression(
                C=0.1,  # ì •ê·œí™” ê°•í™”
                penalty='l2',
                class_weight='balanced',
                random_state=42,
                max_iter=500  # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì¤„ì„
            )
        }
        
        # íŒŒì´í”„ë¼ì¸ë“¤ êµ¬ì¶• (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë§Œ ì‚¬ìš©)
        self.pipelines = {}
        for name, classifier in classifiers.items():
            self.pipelines[name] = Pipeline([
                ('preprocessor', self.preprocessor),
                ('classifier', classifier)
            ])
        
        print(f"  êµ¬ì¶•ëœ íŒŒì´í”„ë¼ì¸ ìˆ˜: {len(self.pipelines)}")
        
    def perform_hyperparameter_optimization(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì˜¤ë²„í”¼íŒ… ë°©ì§€)"""
        
        print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")
        
        X_train = self.train_df.drop('Class', axis=1)
        y_train = self.train_df['Class']
        
        best_pipeline = None
        best_score = 0
        best_name = None
        
        for name, pipeline in self.pipelines.items():
            print(f"\n--- {name.upper()} ëª¨ë¸ ìµœì í™” ---")
            
            # Random Forest ë³µì¡ë„ ë¶„ì„
            if name == 'rf':
                param_ranges = {
                    'classifier__n_estimators': [20, 50, 100],  # ë¹ ë¥¸ ì‹¤í–‰
                    'classifier__max_depth': [5, 8, 10]  # ê°„ì†Œí™”
                }
                
                for param_name, param_range in param_ranges.items():
                    optimal_param, overfitting_severity = self.overfitting_preventer.analyze_model_complexity(
                        pipeline, X_train, y_train, param_name, param_range
                    )
                    
                    # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
                    pipeline.set_params(**{param_name: optimal_param})
            
            # êµì°¨ ê²€ì¦ ìˆ˜í–‰
            cv_results = self.overfitting_preventer.perform_robust_cross_validation(
                pipeline, X_train, y_train, 'timeseries'
            )
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ (F1 ìŠ¤ì½”ì–´ ê¸°ì¤€)
            current_score = cv_results['f1']['test_mean']
            if current_score > best_score:
                best_score = current_score
                best_pipeline = pipeline
                best_name = name
        
        self.pipeline = best_pipeline
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_name.upper()} (F1: {best_score:.3f})")
        
        return best_pipeline
    
    def train_final_model(self):
        """ìµœì¢… ëª¨ë¸ í›ˆë ¨"""
        
        print("ğŸ“ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        if self.pipeline is None:
            raise ValueError("íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        X_train = self.train_df.drop('Class', axis=1)
        y_train = self.train_df['Class']
        
        # ìµœì¢… ëª¨ë¸ í›ˆë ¨
        self.final_model = self.pipeline.fit(X_train, y_train)
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(self.final_model, self.model_save_path)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_save_path}")
        
        return self.final_model
    
    def evaluate_model(self):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        if self.final_model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        X_test = self.test_df.drop('Class', axis=1)
        y_test = self.test_df['Class']
        
        # ì˜ˆì¸¡
        y_pred = self.final_model.predict(X_test)
        y_pred_proba = self.final_model.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        self.performance_metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'average_precision': average_precision_score(y_test, y_pred_proba),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }
        
        # í´ë˜ìŠ¤ë³„ ë¶„í¬
        test_class_dist = np.bincount(y_test)
        pred_class_dist = np.bincount(y_pred)
        
        self.performance_metrics.update({
            'test_fraud_rate': test_class_dist[1] / len(y_test) * 100,
            'predicted_fraud_rate': pred_class_dist[1] / len(y_pred) * 100,
            'total_samples': len(y_test),
            'test_fraud_samples': int(test_class_dist[1]),
            'test_normal_samples': int(test_class_dist[0])
        })
        
        # ê²°ê³¼ ì¶œë ¥
        print("ğŸ“ˆ ìµœì¢… ì„±ëŠ¥ ì§€í‘œ:")
        print(f"  ì •í™•ë„: {self.performance_metrics['accuracy']:.4f}")
        print(f"  ì •ë°€ë„: {self.performance_metrics['precision']:.4f}")
        print(f"  ì¬í˜„ìœ¨: {self.performance_metrics['recall']:.4f}")
        print(f"  F1-ì ìˆ˜: {self.performance_metrics['f1_score']:.4f}")
        print(f"  ROC-AUC: {self.performance_metrics['roc_auc']:.4f}")
        print(f"  í‰ê·  ì •ë°€ë„: {self.performance_metrics['average_precision']:.4f}")
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬:")
        print(f"  ì‹¤ì œ ì‚¬ê¸°ìœ¨: {self.performance_metrics['test_fraud_rate']:.3f}%")
        print(f"  ì˜ˆì¸¡ ì‚¬ê¸°ìœ¨: {self.performance_metrics['predicted_fraud_rate']:.3f}%")
        
        # í˜¼ë™ í–‰ë ¬
        cm = self.performance_metrics['confusion_matrix']
        print(f"\nğŸ” í˜¼ë™ í–‰ë ¬:")
        print(f"  ì •ìƒâ†’ì •ìƒ: {cm[0][0]:,}, ì •ìƒâ†’ì‚¬ê¸°: {cm[0][1]:,}")
        print(f"  ì‚¬ê¸°â†’ì •ìƒ: {cm[1][0]:,}, ì‚¬ê¸°â†’ì‚¬ê¸°: {cm[1][1]:,}")
        
        return self.performance_metrics
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'model_info': {
                'data_path': self.data_path,
                'model_save_path': self.model_save_path,
                'training_samples': len(self.train_df) if self.train_df is not None else 0,
                'test_samples': len(self.test_df) if self.test_df is not None else 0
            },
            'data_splitting': self.temporal_splitter.split_info,
            'validation_report': self.validation_report,
            'cross_validation_results': self.overfitting_preventer.cross_val_results,
            'performance_metrics': self.performance_metrics,
            'security_status': {
                'data_leakage_prevented': len(self.leakage_validator.critical_issues) == 0,
                'temporal_split_used': True,
                'pipeline_isolation': True,
                'cross_validation_applied': len(self.overfitting_preventer.cross_val_results) > 0,
                'overfitting_prevented': True,
                'class_balancing_applied': True
            }
        }
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.model_save_path.replace('.pkl', '_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"ğŸ“‹ ì¢…í•© ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        return report
    
    def run_complete_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        print("ğŸš€ ì—„ê²©í•œ ì‚¬ê¸°íƒì§€ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
            df = self.load_and_validate_data()
            
            # 2. ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• 
            self.split_data_temporally(df)
            
            # 3. ë³´ì•ˆ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
            self.build_secure_pipeline()
            
            # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            self.perform_hyperparameter_optimization()
            
            # 5. ìµœì¢… ëª¨ë¸ í›ˆë ¨
            self.train_final_model()
            
            # 6. ëª¨ë¸ í‰ê°€
            self.evaluate_model()
            
            # 7. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.generate_comprehensive_report()
            
            print("\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print(f"ë³´ì•ˆ ì ìˆ˜: {'SECURE' if report['security_status']['data_leakage_prevented'] else 'INSECURE'}")
            
            return report
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise e


if __name__ == "__main__":
    # ì‚¬ê¸°íƒì§€ ëª¨ë¸ ì‹¤í–‰ (ìƒ˜í”Œë§ ë²„ì „)
    data_path = "/root/FCA/data/wamc_fraud/wamc_fraud_processed.csv"
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘
    print("ğŸš€ ì—„ê²©í•œ ì‚¬ê¸°íƒì§€ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ìƒ˜í”Œë§ ë²„ì „)")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§
    import pandas as pd
    df = pd.read_csv(data_path)
    print(f"ì›ë³¸ ë°ì´í„°: {len(df):,}ê°œ ìƒ˜í”Œ")
    
    # 50,000ê°œ ìƒ˜í”Œë¡œ ì œí•œ (ì„±ëŠ¥ í™•ì¸ìš©)
    if len(df) > 50000:
        step = len(df) // 50000
        df_sampled = df.iloc[::step][:50000]
        print(f"ìƒ˜í”Œë§ëœ ë°ì´í„°: {len(df_sampled):,}ê°œ ìƒ˜í”Œ")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_path = "/root/FCA/data/wamc_fraud/wamc_fraud_sampled.csv"
        df_sampled.to_csv(temp_path, index=False)
        
        detector = SecureFraudDetector(temp_path)
    else:
        detector = SecureFraudDetector(data_path)
    
    report = detector.run_complete_pipeline()
    
    print("\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
    print(f"F1-ì ìˆ˜: {report['performance_metrics']['f1_score']:.4f}")
    print(f"ROC-AUC: {report['performance_metrics']['roc_auc']:.4f}")
    print(f"ë°ì´í„° ëˆ„ì¶œ ë°©ì§€: {'âœ…' if report['security_status']['data_leakage_prevented'] else 'âŒ'}")
    print(f"ì˜¤ë²„í”¼íŒ… ë°©ì§€: {'âœ…' if report['security_status']['overfitting_prevented'] else 'âŒ'}")