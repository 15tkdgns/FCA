#!/usr/bin/env python3
"""
ì‚¬ê¸°íƒì§€ ë°ì´í„° ëˆ„ì¶œ ë° ì˜¤ë²„í”¼íŒ… ì—„ê²© ê²€ì¦ ëª¨ë“ˆ
=============================================

ë°ì´í„° ëˆ„ì¶œ(Data Leakage)ê³¼ ì˜¤ë²„í”¼íŒ…(Overfitting) ê²€ì¦ì„ ìœ„í•œ 
ì—„ê²©í•œ ê²€ì¦ í”„ë ˆì„ì›Œí¬
"""

import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any

warnings.filterwarnings('ignore')


class FraudValidationFramework:
    """ì‚¬ê¸°íƒì§€ ë°ì´í„° ëˆ„ì¶œ ë° ì˜¤ë²„í”¼íŒ… ê²€ì¦ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.df = None
        self.validation_results = {}
        self.critical_issues = []
        self.warnings = []
        
    def load_and_analyze_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„"""
        print("ğŸ” ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ë¶„ì„ ì¤‘...")
        
        try:
            self.df = pd.read_csv(self.data_path)
            
            analysis = {
                'total_samples': len(self.df),
                'total_features': len(self.df.columns) - 1,  # íƒ€ê²Ÿ ì œì™¸
                'fraud_samples': sum(self.df['Class'] == 1),
                'normal_samples': sum(self.df['Class'] == 0),
                'fraud_rate': sum(self.df['Class'] == 1) / len(self.df) * 100,
                'missing_values': self.df.isnull().sum().sum(),
                'duplicated_rows': self.df.duplicated().sum()
            }
            
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {analysis['total_samples']:,}ê°œ ìƒ˜í”Œ")
            return analysis
            
        except Exception as e:
            self.critical_issues.append(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise e
    
    def check_temporal_data_leakage(self) -> Dict[str, Any]:
        """ì‹œê°„ì  ë°ì´í„° ëˆ„ì¶œ ê²€ì¦"""
        print("â° ì‹œê°„ì  ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì¤‘...")
        
        temporal_check = {
            'has_time_column': False,
            'time_sorted': False,
            'future_data_leak': False,
            'temporal_split_used': False,
            'severity': 'LOW'
        }
        
        # Time ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        if 'Time' in self.df.columns:
            temporal_check['has_time_column'] = True
            
            # ì‹œê°„ ìˆœì„œ í™•ì¸
            time_col = self.df['Time']
            if time_col.is_monotonic_increasing:
                temporal_check['time_sorted'] = True
                print("âœ… ì‹œê°„ ë°ì´í„°ê°€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë¨")
            else:
                temporal_check['time_sorted'] = False
                self.critical_issues.append("âŒ ì‹œê°„ ë°ì´í„°ê°€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì§€ ì•ŠìŒ - ì‹¬ê°í•œ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜")
                temporal_check['severity'] = 'CRITICAL'
            
            # ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ ìœ„í—˜ í™•ì¸
            # ì¼ë°˜ì ì¸ train_test_splitì€ ì‹œê°„ ìˆœì„œë¥¼ ë¬´ì‹œí•¨
            temporal_check['future_data_leak'] = True
            self.critical_issues.append("âŒ ì‹œê°„ ìˆœì„œë¥¼ ë¬´ì‹œí•œ random split ì‚¬ìš© - ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ")
            temporal_check['severity'] = 'CRITICAL'
            
        else:
            print("âš ï¸ Time ì»¬ëŸ¼ì´ ì—†ìŒ - ì‹œê°„ì  ê²€ì¦ ë¶ˆê°€")
            self.warnings.append("Time ì»¬ëŸ¼ ë¶€ì¬ë¡œ ì‹œê°„ì  ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ë¶ˆê°€")
        
        return temporal_check
    
    def check_feature_leakage(self) -> Dict[str, Any]:
        """íŠ¹ì„± ê¸°ë°˜ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦"""
        print("ğŸ”¬ íŠ¹ì„± ê¸°ë°˜ ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì¤‘...")
        
        feature_leakage = {
            'perfect_predictors': [],
            'highly_correlated_features': [],
            'constant_features': [],
            'near_constant_features': [],
            'severity': 'LOW'
        }
        
        if self.df is None:
            return feature_leakage
        
        X = self.df.drop('Class', axis=1)
        y = self.df['Class']
        
        # 1. ì™„ë²½í•œ ì˜ˆì¸¡ì ì°¾ê¸° (ë‹¨ì¼ íŠ¹ì„±ìœ¼ë¡œ 100% ë¶„ë¥˜)
        for col in X.columns:
            if X[col].dtype in ['int64', 'float64']:
                # ê° íŠ¹ì„±ë³„ë¡œ ë‹¨ìˆœ ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸
                thresholds = np.percentile(X[col], [25, 50, 75, 90, 95, 99])
                for threshold in thresholds:
                    pred_above = (X[col] > threshold).astype(int)
                    pred_below = (X[col] <= threshold).astype(int)
                    
                    acc_above = accuracy_score(y, pred_above)
                    acc_below = accuracy_score(y, pred_below)
                    
                    if acc_above > 0.99 or acc_below > 0.99:
                        feature_leakage['perfect_predictors'].append({
                            'feature': col,
                            'threshold': threshold,
                            'accuracy': max(acc_above, acc_below)
                        })
                        self.critical_issues.append(f"âŒ ì™„ë²½í•œ ì˜ˆì¸¡ì ë°œê²¬: {col} (ì •í™•ë„: {max(acc_above, acc_below):.4f})")
                        feature_leakage['severity'] = 'CRITICAL'
        
        # 2. íƒ€ê²Ÿê³¼ ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„±
        for col in X.columns:
            if X[col].dtype in ['int64', 'float64']:
                correlation = abs(np.corrcoef(X[col], y)[0, 1])
                if correlation > 0.9:
                    feature_leakage['highly_correlated_features'].append({
                        'feature': col,
                        'correlation': correlation
                    })
                    if correlation > 0.95:
                        self.critical_issues.append(f"âŒ ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„: {col} (r={correlation:.4f})")
                        feature_leakage['severity'] = 'CRITICAL'
        
        # 3. ìƒìˆ˜ íŠ¹ì„± (ë°ì´í„° ëˆ„ì¶œì€ ì•„ë‹ˆì§€ë§Œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ)
        for col in X.columns:
            unique_values = X[col].nunique()
            if unique_values == 1:
                feature_leakage['constant_features'].append(col)
            elif unique_values <= 3 and len(X) > 1000:
                feature_leakage['near_constant_features'].append({
                    'feature': col,
                    'unique_values': unique_values
                })
        
        return feature_leakage
    
    def check_statistical_leakage(self) -> Dict[str, Any]:
        """í†µê³„ì  ë°ì´í„° ëˆ„ì¶œ ê²€ì¦"""
        print("ğŸ“Š í†µê³„ì  ë°ì´í„° ëˆ„ì¶œ ê²€ì¦ ì¤‘...")
        
        statistical_check = {
            'target_leakage_features': [],
            'preprocessing_leakage': False,
            'scaling_before_split': False,
            'severity': 'LOW'
        }
        
        if self.df is None:
            return statistical_check
        
        X = self.df.drop('Class', axis=1)
        y = self.df['Class']
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ìŠ¤ì¼€ì¼ë§ì´ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        # (ì‹¤ì œë¡œëŠ” train setì—ë§Œ fití•´ì•¼ í•¨)
        for col in X.columns:
            if X[col].dtype in ['int64', 'float64']:
                # íŠ¹ì„±ì´ ì •ê·œí™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (í‰ê· â‰ˆ0, í‘œì¤€í¸ì°¨â‰ˆ1)
                mean_val = X[col].mean()
                std_val = X[col].std()
                
                if abs(mean_val) < 0.1 and abs(std_val - 1.0) < 0.1:
                    statistical_check['scaling_before_split'] = True
                    self.critical_issues.append(f"âŒ ì „ì²´ ë°ì´í„°ì— ìŠ¤ì¼€ì¼ë§ ì ìš© ì˜ì‹¬: {col}")
                    statistical_check['severity'] = 'CRITICAL'
                    break
        
        return statistical_check
    
    def perform_proper_validation(self) -> Dict[str, Any]:
        """ì˜¬ë°”ë¥¸ ê²€ì¦ ë°©ë²•ë¡  ì ìš©"""
        print("âœ… ì˜¬ë°”ë¥¸ ê²€ì¦ ë°©ë²•ë¡  ì ìš© ì¤‘...")
        
        if self.df is None:
            return {}
        
        X = self.df.drop('Class', axis=1)
        y = self.df['Class']
        
        # ì‹œê°„ ìˆœì„œë¥¼ ê³ ë ¤í•œ ë¶„í•  (Time ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
        validation_results = {
            'random_split_results': {},
            'temporal_split_results': {},
            'cross_validation_results': {},
            'overfitting_analysis': {}
        }
        
        # 1. ì˜ëª»ëœ ë°©ë²•: Random Split (í˜„ì¬ ì½”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•)
        print("ğŸ”´ ì˜ëª»ëœ ë°©ë²•: Random Split í…ŒìŠ¤íŠ¸")
        validation_results['random_split_results'] = self._test_random_split(X, y)
        
        # 2. ì˜¬ë°”ë¥¸ ë°©ë²•: Temporal Split (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
        if 'Time' in self.df.columns:
            print("ğŸŸ¢ ì˜¬ë°”ë¥¸ ë°©ë²•: Temporal Split í…ŒìŠ¤íŠ¸")
            validation_results['temporal_split_results'] = self._test_temporal_split(X, y)
        
        # 3. êµì°¨ ê²€ì¦
        print("ğŸ”µ êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸")
        validation_results['cross_validation_results'] = self._test_cross_validation(X, y)
        
        # 4. ì˜¤ë²„í”¼íŒ… ë¶„ì„
        print("ğŸ“ˆ ì˜¤ë²„í”¼íŒ… ë¶„ì„")
        validation_results['overfitting_analysis'] = self._analyze_overfitting(X, y)
        
        return validation_results
    
    def _test_random_split(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """Random Split í…ŒìŠ¤íŠ¸ (í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì˜ëª»ëœ ë°©ë²•)"""
        from sklearn.model_selection import train_test_split
        
        # í˜„ì¬ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ì˜ëª»ëœ ë°©ì‹: ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ëª¨ë¸ í›ˆë ¨
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        # í›ˆë ¨ ì„¸íŠ¸ ì„±ëŠ¥ë„ í™•ì¸ (ì˜¤ë²„í”¼íŒ… ê²€ì¦ìš©)
        y_train_pred = model.predict(X_train_scaled)
        
        return {
            'test_accuracy': accuracy_score(y_test, y_pred),
            'test_precision': precision_score(y_test, y_pred, zero_division=0),
            'test_recall': recall_score(y_test, y_pred, zero_division=0),
            'test_f1': f1_score(y_test, y_pred, zero_division=0),
            'test_auc': roc_auc_score(y_test, y_pred_proba),
            'train_accuracy': accuracy_score(y_train, y_train_pred),
            'overfitting_gap': accuracy_score(y_train, y_train_pred) - accuracy_score(y_test, y_pred)
        }
    
    def _test_temporal_split(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """Temporal Split í…ŒìŠ¤íŠ¸ (ì˜¬ë°”ë¥¸ ì‹œê°„ ìˆœì„œ ê³ ë ¤ ë°©ë²•)"""
        if 'Time' not in X.columns:
            return {}
        
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        time_sorted_idx = X['Time'].argsort()
        X_sorted = X.iloc[time_sorted_idx]
        y_sorted = y.iloc[time_sorted_idx]
        
        # ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ì²˜ìŒ 80%ë¥¼ í›ˆë ¨, ë‚˜ë¨¸ì§€ 20%ë¥¼ í…ŒìŠ¤íŠ¸)
        split_idx = int(len(X_sorted) * 0.8)
        
        X_train = X_sorted.iloc[:split_idx]
        X_test = X_sorted.iloc[split_idx:]
        y_train = y_sorted.iloc[:split_idx]
        y_test = y_sorted.iloc[split_idx:]
        
        # ì˜¬ë°”ë¥¸ ë°©ì‹: í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ…
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ëª¨ë¸ í›ˆë ¨
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        # í›ˆë ¨ ì„¸íŠ¸ ì„±ëŠ¥
        y_train_pred = model.predict(X_train_scaled)
        
        return {
            'test_accuracy': accuracy_score(y_test, y_pred),
            'test_precision': precision_score(y_test, y_pred, zero_division=0),
            'test_recall': recall_score(y_test, y_pred, zero_division=0),
            'test_f1': f1_score(y_test, y_pred, zero_division=0),
            'test_auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0.5,
            'train_accuracy': accuracy_score(y_train, y_train_pred),
            'overfitting_gap': accuracy_score(y_train, y_train_pred) - accuracy_score(y_test, y_pred),
            'train_fraud_rate': sum(y_train) / len(y_train) * 100,
            'test_fraud_rate': sum(y_test) / len(y_test) * 100
        }
    
    def _test_cross_validation(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        
        # TimeSeriesSplit ì‚¬ìš© (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
        if 'Time' in X.columns:
            cv = TimeSeriesSplit(n_splits=5)
            cv_name = "TimeSeriesSplit"
        else:
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            cv_name = "StratifiedKFold"
        
        # ì—¬ëŸ¬ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=50, random_state=42),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
        }
        
        cv_results = {'cv_method': cv_name, 'model_results': {}}
        
        for model_name, model in models.items():
            scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)
            cv_results['model_results'][model_name] = {
                'mean_accuracy': scores.mean(),
                'std_accuracy': scores.std(),
                'min_accuracy': scores.min(),
                'max_accuracy': scores.max(),
                'scores': scores.tolist()
            }
        
        return cv_results
    
    def _analyze_overfitting(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ì˜¤ë²„í”¼íŒ… ìƒì„¸ ë¶„ì„"""
        
        # í•™ìŠµ ê³¡ì„  ë¶„ì„
        train_sizes = np.linspace(0.1, 1.0, 10)
        train_scores = []
        test_scores = []
        
        for train_size in train_sizes:
            # ë°ì´í„° ìƒ˜í”Œë§
            sample_size = int(len(X) * train_size)
            X_sample = X.iloc[:sample_size]
            y_sample = y.iloc[:sample_size]
            
            # Train/Test ë¶„í• 
            split_idx = int(len(X_sample) * 0.8)
            X_train = X_sample.iloc[:split_idx]
            X_test = X_sample.iloc[split_idx:]
            y_train = y_sample.iloc[:split_idx]
            y_test = y_sample.iloc[split_idx:]
            
            if len(X_train) < 10 or len(X_test) < 10:
                continue
            
            # ìŠ¤ì¼€ì¼ë§
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ëª¨ë¸ í›ˆë ¨
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X_train_scaled, y_train)
            
            # ì„±ëŠ¥ ì¸¡ì •
            train_acc = model.score(X_train_scaled, y_train)
            test_acc = model.score(X_test_scaled, y_test)
            
            train_scores.append(train_acc)
            test_scores.append(test_acc)
        
        # ì˜¤ë²„í”¼íŒ… ì§€í‘œ ê³„ì‚°
        if train_scores and test_scores:
            avg_gap = np.mean([t - v for t, v in zip(train_scores, test_scores)])
            max_gap = max([t - v for t, v in zip(train_scores, test_scores)])
            
            overfitting_severity = 'LOW'
            if avg_gap > 0.1:
                overfitting_severity = 'HIGH'
            elif avg_gap > 0.05:
                overfitting_severity = 'MEDIUM'
        else:
            avg_gap = 0
            max_gap = 0
            overfitting_severity = 'UNKNOWN'
        
        return {
            'train_scores': train_scores,
            'test_scores': test_scores,
            'average_overfitting_gap': avg_gap,
            'max_overfitting_gap': max_gap,
            'overfitting_severity': overfitting_severity,
            'final_train_accuracy': train_scores[-1] if train_scores else 0,
            'final_test_accuracy': test_scores[-1] if test_scores else 0
        }
    
    def generate_validation_report(self) -> Dict[str, Any]:
        """ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ì „ì²´ ê²€ì¦ ì‹¤í–‰
        data_analysis = self.load_and_analyze_data()
        temporal_check = self.check_temporal_data_leakage()
        feature_check = self.check_feature_leakage()
        statistical_check = self.check_statistical_leakage()
        validation_results = self.perform_proper_validation()
        
        # ì‹¬ê°ë„ ê³„ì‚°
        severity_levels = [
            temporal_check.get('severity', 'LOW'),
            feature_check.get('severity', 'LOW'),
            statistical_check.get('severity', 'LOW'),
        ]
        
        overall_severity = 'LOW'
        if 'CRITICAL' in severity_levels:
            overall_severity = 'CRITICAL'
        elif 'HIGH' in severity_levels:
            overall_severity = 'HIGH'
        elif 'MEDIUM' in severity_levels:
            overall_severity = 'MEDIUM'
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (10ì  ë§Œì )
        score = 10.0
        score -= len(self.critical_issues) * 2.0  # ì‹¬ê°í•œ ë¬¸ì œë‹¹ -2ì 
        score -= len(self.warnings) * 0.5        # ê²½ê³ ë‹¹ -0.5ì 
        score = max(0.0, score)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'overall_severity': overall_severity,
            'validation_score': round(score, 2),
            'max_score': 10.0,
            'data_analysis': data_analysis,
            'temporal_check': temporal_check,
            'feature_check': feature_check,
            'statistical_check': statistical_check,
            'validation_results': validation_results,
            'critical_issues': self.critical_issues,
            'warnings': self.warnings,
            'recommendations': self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if any('ì‹œê°„' in issue for issue in self.critical_issues):
            recommendations.append("ğŸ• TimeSeriesSplit ë˜ëŠ” ì‹œê°„ ê¸°ë°˜ ë¶„í•  ì‚¬ìš© í•„ìˆ˜")
            recommendations.append("ğŸš« Random train_test_split ì‚¬ìš© ê¸ˆì§€")
        
        if any('ì™„ë²½í•œ ì˜ˆì¸¡ì' in issue for issue in self.critical_issues):
            recommendations.append("ğŸ” ì™„ë²½í•œ ì˜ˆì¸¡ì íŠ¹ì„± ì œê±° ë˜ëŠ” ë³„ë„ ê²€ì¦")
            recommendations.append("ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ë° ìƒê´€ê´€ê³„ ì¬ë¶„ì„")
        
        if any('ìŠ¤ì¼€ì¼ë§' in issue for issue in self.critical_issues):
            recommendations.append("âš™ï¸ í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ…, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” transformë§Œ ì ìš©")
            recommendations.append("ğŸ”§ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©ìœ¼ë¡œ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€")
        
        recommendations.extend([
            "ğŸ“ˆ êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì•ˆì •ì„± í™•ì¸",
            "ğŸ“‰ í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ ì˜¤ë²„í”¼íŒ… ëª¨ë‹ˆí„°ë§",
            "ğŸ¯ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° (SMOTE, class_weight ë“±)",
            "ğŸ”„ ëª¨ë¸ ë³µì¡ë„ ì¡°ì ˆ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)"
        ])
        
        return recommendations


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def validate_fraud_detection_system(data_path: str) -> Dict[str, Any]:
    """ì‚¬ê¸°íƒì§€ ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦"""
    print("ğŸš¨ ì‚¬ê¸°íƒì§€ ì‹œìŠ¤í…œ ì—„ê²© ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    
    validator = FraudValidationFramework(data_path)
    report = validator.generate_validation_report()
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
    print(f"ì „ì²´ ì‹¬ê°ë„: {report['overall_severity']}")
    print(f"ê²€ì¦ ì ìˆ˜: {report['validation_score']}/{report['max_score']}")
    print(f"ì‹¬ê°í•œ ë¬¸ì œ: {len(report['critical_issues'])}ê°œ")
    print(f"ê²½ê³ : {len(report['warnings'])}ê°œ")
    
    if report['critical_issues']:
        print(f"\nâŒ ì‹¬ê°í•œ ë¬¸ì œë“¤:")
        for issue in report['critical_issues']:
            print(f"  {issue}")
    
    if report['warnings']:
        print(f"\nâš ï¸ ê²½ê³ ì‚¬í•­ë“¤:")
        for warning in report['warnings']:
            print(f"  {warning}")
    
    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    for rec in report['recommendations']:
        print(f"  {rec}")
    
    return report


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    data_path = "/root/FCA/data/wamc_fraud/wamc_fraud_processed.csv"
    report = validate_fraud_detection_system(data_path)