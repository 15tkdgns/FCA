<?xml version="1.0" encoding="UTF-8"?>
<ProjectWorkflow>
    <DataCollection>
        <Phase name="Data Collection" description="Gathering raw data from various sources.">
            <Step id="DC001" name="Project Folder Setup" status="Completed">
                <Description>Created a structured project directory for backend, frontend, data, notebooks, docker, scripts, and docs.</Description>
                <ToolUsed>mkdir -p backend frontend data notebooks docker scripts docs</ToolUsed>
                <Input>User request for project structure.</Input>
                <Output>Created directories: backend/, data/, docs/, docker/, frontend/, notebooks/, scripts/.</Output>
                <Rationale>Organize project files for better management and modularity.</Rationale>
            </Step>
            <Step id="DC002" name="Kaggle API Setup" status="Completed">
                <Description>Installed 'kaggle' library in a virtual environment and configured 'kaggle.json' for API authentication.</Description>
                <ToolUsed>pip install kaggle (in venv), python3 -m venv, mv, chmod</ToolUsed>
                <Input>User request to download Kaggle datasets, kaggle.json file.</Input>
                <Output>Kaggle library installed, kaggle.json moved to ~/.kaggle/ and permissions set to 600.</Output>
                <Rationale>Enable programmatic download of Kaggle datasets, adhering to security best practices for API keys.</Rationale>
            </Step>
            <Step id="DC003" name="Download Online Retail II UCI Dataset" status="Completed">
                <Description>Downloaded the 'mashlyn/online-retail-ii-uci' dataset from Kaggle using the Kaggle API.</Description>
                <ToolUsed>kaggle.api.dataset_download_files (via Python script: scripts/download_kaggle_dataset.py)</ToolUsed>
                <Input>Dataset ID: mashlyn/online-retail-ii-uci.</Input>
                <Output>online_retail_II.csv in /root/FCA/data/online_retail_ii.</Output>
                <Rationale>Acquire a comprehensive financial transaction dataset for retail consumer behavior analysis.</Rationale>
            </Step>
            <Step id="DC004" name="Download Credit Card Fraud Detection Dataset (mlg-ulb)" status="Completed">
                <Description>Downloaded the 'mlg-ulb/creditcardfraud' dataset from Kaggle using the Kaggle API.</Description>
                <ToolUsed>kaggle.api.dataset_download_files (via Python script: scripts/download_creditcardfraud_dataset.py)</ToolUsed>
                <Input>Dataset ID: mlg-ulb/creditcardfraud.</Input>
                <Output>creditcard.csv (or similar) in /root/FCA/data/creditcardfraud.</Output>
                <Rationale>Acquire a classic credit card fraud detection dataset for anomaly detection modeling.</Rationale>
            </Step>
            <Step id="DC005" name="Download Credit Card Fraud Detection Dataset 2023 (nelgiriyewithana)" status="Completed">
                <Description>Downloaded the 'nelgiriyewithana/credit-card-fraud-detection-dataset-2023' dataset from Kaggle using the Kaggle API.</Description>
                <ToolUsed>kaggle.api.dataset_download_files (via Python script: scripts/download_creditcardfraud_2023_dataset.py)</ToolUsed>
                <Input>Dataset ID: nelgiriyewithana/credit-card-fraud-detection-dataset-2023.</Input>
                <Output>creditcard_2023.csv (or similar) in /root/FCA/data/credit_card_fraud_2023.</Output>
                <Rationale>Acquire a more recent credit card fraud detection dataset to compare with older datasets and potentially capture newer fraud patterns.</Rationale>
            </Step>
            <Step id="DC006" name="Load HuggingFace Credit Card Fraud Dataset (dazzle-nu)" status="Completed">
                <Description>Loaded 'dazzle-nu/CIS435-CreditCardFraudDetection' from HuggingFace using the 'datasets' library and converted it to a Pandas DataFrame.</Description>
                <ToolUsed>datasets.load_dataset, pandas.DataFrame.to_pandas (via Python script: scripts/load_hf_dataset_to_df.py)</ToolUsed>
                <Input>Dataset ID: dazzle-nu/CIS435-CreditCardFraudDetection.</Input>
                <Output>Pandas DataFrame (head printed to console).</Output>
                <Rationale>Explore an additional source of credit card fraud data, leveraging HuggingFace's dataset ecosystem.</Rationale>
            </Step>
        </Phase>
    </DataCollection>

    <DataPreprocessing>
        <Phase name="Data Preprocessing" description="Cleaning, transforming, and preparing data for analysis and modeling.">
            <Step id="DP001" name="Initial Data Exploration (Online Retail II)" status="Completed">
                <Description>Loaded 'online_retail_II.csv' into a Pandas DataFrame and displayed its head() and info() to understand data structure, types, and identify missing values.</Description>
                <ToolUsed>pandas.read_csv, df.head(), df.info()</ToolUsed>
                <Input>/root/FCA/data/online_retail_ii/online_retail_II.csv</Input>
                <Output>Printed DataFrame head and info, revealing missing values in 'Description' and 'Customer ID', and 'InvoiceDate' as object type.</Output>
                <Rationale>Gain initial insights into the dataset's characteristics, identify data quality issues, and plan subsequent preprocessing steps.</Rationale>
            </Step>
            <Step id="DP002" name="Convert InvoiceDate to Datetime (Online Retail II)" status="Completed">
                <Description>Converted the 'InvoiceDate' column from object to datetime type for proper time-series analysis and feature extraction.</Description>
                <ToolUsed>pandas.to_datetime</ToolUsed>
                <Input>InvoiceDate column of Online Retail II DataFrame.</Input>
                <Output>InvoiceDate column with datetime objects.</Output>
                <Rationale>Enabled accurate time-based analysis (e.g., trends, seasonality) and facilitated the creation of time-derived features.</Rationale>
            </Step>
            <Step id="DP003" name="Handle Missing Customer ID (Online Retail II)" status="Completed">
                <Description>Filled missing values in the 'Customer ID' column with 0 and converted the column to integer type. This allows for inclusion of non-registered customer transactions while distinguishing them.</Description>
                <ToolUsed>df.fillna(0).astype(int)</ToolUsed>
                <Input>Customer ID column of Online Retail II DataFrame.</Input>
                <Output>Customer ID column with no missing values and integer type.</Output>
                <Rationale>Ensure data integrity for customer-centric analysis and prevent errors in downstream processes, while retaining information about transactions without a registered customer ID.</Rationale>
            </Step>
            <Step id="DP004" name="Handle Missing Description (Online Retail II)" status="Completed">
                <Description>Filled missing values in the 'Description' column with 'Unknown'. This ensures all transactions have a description, even if generic.</Description>
                <ToolUsed>df.fillna('Unknown')</ToolUsed>
                <Input>Description column of Online Retail II DataFrame.</Input>
                <Output>Description column with no missing values.</Output>
                <Rationale>Maintain data quality for product-centric analysis and avoid issues with categorical data processing, allowing for analysis of transactions with unknown product descriptions.</Rationale>
            </Step>
            <Step id="DP005" name="Handle Negative Quantity (Online Retail II)" status="Completed">
                <Description>Separated the DataFrame into 'sales_df' (Quantity &gt; 0) and 'returns_df' (Quantity &lt; 0) to distinguish between sales and returns for separate analysis.</Description>
                <ToolUsed>Conditional filtering (df[df['Quantity'] &gt; 0], df[df['Quantity'] &lt; 0])</ToolUsed>
                <Input>Quantity column of Online Retail II DataFrame.</Input>
                <Output>Two DataFrames: sales_df and returns_df.</Output>
                <Rationale>Accurately reflect sales transactions and enable separate analysis of return patterns, which are crucial for understanding customer behavior and product performance.</Rationale>
            </Step>
            <Step id="DP006" name="Handle Cancelled Orders (Online Retail II)" status="Completed">
                <Description>Separated the DataFrame into 'sales_df' (non-cancelled orders) and 'cancelled_df' (orders with 'Invoice' starting with 'C') to distinguish between valid sales and cancellations.</Description>
                <ToolUsed>String matching (df['Invoice'].astype(str).str.startswith('C')), conditional filtering</ToolUsed>
                <Input>Invoice column of Online Retail II DataFrame.</Input>
                <Output>Two DataFrames: sales_df (non-cancelled) and cancelled_df.</Output>
                <Rationale>Distinguish valid sales from cancellations to ensure accurate revenue and transaction analysis, and enable separate analysis of cancellation rates.</Rationale>
            </Step>
            <Step id="DP007" name="Feature Engineering (Online Retail II)" status="Completed">
                <Description>Created 'TotalPrice' (Quantity * Price) and extracted time-based features ('Year', 'Month', 'Day', 'Hour') from 'InvoiceDate'.</Description>
                <ToolUsed>Pandas arithmetic operations, datetime attributes</ToolUsed>
                <Input>Quantity, Price, InvoiceDate columns of Online Retail II DataFrame.</Input>
                <Output>DataFrame with new engineered features added: TotalPrice, Year, Month, Day, Hour.</Output>
                <Rationale>Enhanced the dataset with more informative variables for deeper insights into consumer behavior and improved model performance.</Rationale>
            </Step>
            <Step id="DP008" name="Initial Data Exploration (Credit Card Fraud)" status="Completed">
                <Description>Loaded 'creditcard.csv' into a Pandas DataFrame and displayed its head() and info() to understand data structure, types, and identify missing values.</Description>
                <ToolUsed>pandas.read_csv, df.head(), df.info()</ToolUsed>
                <Input>/root/FCA/data/creditcardfraud/creditcard.csv</Input>
                <Output>Printed DataFrame head and info, revealing no missing values and pre-processed V-features.</Output>
                <Rationale>Gain initial insights into the dataset's characteristics and plan subsequent preprocessing steps, noting the already anonymized and scaled V-features.</Rationale>
            </Step>
            <Step id="DP009" name="Check Class Imbalance (Credit Card Fraud)" status="Completed">
                <Description>Analyzed the distribution of the 'Class' column to identify class imbalance, which is critical for fraud detection datasets.</Description>
                <ToolUsed>df['Class'].value_counts()</ToolUsed>
                <Input>Class column of Credit Card Fraud DataFrame.</Input>
                <Output>Printed value counts and normalized percentages, confirming severe class imbalance (approx. 99.8% normal, 0.2% fraud).</Output>
                <Rationale>Understand the target variable distribution to plan appropriate modeling strategies (e.g., oversampling, undersampling, weighted loss) for imbalanced classification.</Rationale>
            </Step>
            <Step id="DP010" name="Initial Data Exploration (Credit Card Fraud 2023)" status="Completed">
                <Description>Loaded 'creditcard_2023.csv' into a Pandas DataFrame and displayed its head() and info() to understand data structure, types, and identify missing values.</Description>
                <ToolUsed>pandas.read_csv, df.head(), df.info()</ToolUsed>
                <Input>/root/FCA/data/credit_card_fraud_2023/creditcard_2023.csv</Input>
                <Output>Printed DataFrame head and info, revealing no missing values and pre-processed V-features, and an 'id' column.</Output>
                <Rationale>Gain initial insights into the dataset's characteristics and plan subsequent preprocessing steps, noting the already anonymized and scaled V-features and the presence of an 'id' column.</Rationale>
            </Step>
            <Step id="DP011" name="Check Class Distribution (Credit Card Fraud 2023)" status="Completed">
                <Description>Analyzed the distribution of the 'Class' column to understand the balance between normal and fraudulent transactions.</Description>
                <ToolUsed>df['Class'].value_counts()</ToolUsed>
                <Input>Class column of Credit Card Fraud 2023 DataFrame.</Input>
                <Output>Printed value counts and normalized percentages, confirming a perfectly balanced dataset (50% normal, 50% fraud).</Output>
                <Rationale>Understand the target variable distribution to inform modeling strategies, noting the artificial balancing of this dataset.</Rationale>
            </Step>
            <Step id="DP012" name="Drop 'id' Column (Credit Card Fraud 2023)" status="Completed">
                <Description>Removed the 'id' column from the dataset as it is a unique identifier and not a predictive feature.</Description>
                <ToolUsed>df.drop('id', axis=1)</ToolUsed>
                <Input>Credit Card Fraud 2023 DataFrame with 'id' column.</Input>
                <Output>DataFrame without the 'id' column.</Output>
                <Rationale>Eliminate non-predictive features to simplify the model and prevent potential data leakage or overfitting.</Rationale>
            </Step>
            <Step id="DP013" name="Initial Data Exploration & Class Imbalance (HuggingFace CIS435-CreditCardFraudDetection)" status="Completed">
                <Description>Loaded 'dazzle-nu/CIS435-CreditCardFraudDetection' from HuggingFace, converted to Pandas DataFrame, and analyzed the 'is_fraud' column for class imbalance.</Description>
                <ToolUsed>datasets.load_dataset, pandas.DataFrame.to_pandas, df['is_fraud'].value_counts()</ToolUsed>
                <Input>HuggingFace Dataset ID: dazzle-nu/CIS435-CreditCardFraudDetection.</Input>
                <Output>Printed DataFrame head and info (from previous step), and value counts/percentages for 'is_fraud', confirming severe class imbalance (approx. 99.4% normal, 0.6% fraud).</Output>
                <Rationale>Understand the dataset structure and target variable distribution to plan appropriate preprocessing and modeling strategies for imbalanced classification.</Rationale>
            </Step>
            <Step id="DP014" name="Comprehensive Dataset Analysis (Credit Card Fraud 2023)" status="Completed">
                <Description>Performed comprehensive preprocessing and analysis of the Credit Card Fraud 2023 dataset including duplicate removal, feature analysis, and class distribution validation.</Description>
                <ToolUsed>comprehensive_preprocessing.py, pandas.read_csv, df.drop_duplicates(), df.describe()</ToolUsed>
                <Input>/root/FCA/data/credit_card_fraud_2023/creditcard_2023.csv (568,630 rows × 31 columns)</Input>
                <Output>Generated creditcard_2023_processed.csv (568,629 rows × 30 columns), preprocessing_report.json with detailed statistics</Output>
                <Rationale>Ensure data quality by removing duplicates and non-predictive features, validate the artificially balanced dataset (50% fraud, 50% normal), and extract comprehensive feature statistics for modeling preparation.</Rationale>
            </Step>
            <Step id="DP015" name="Dataset Inventory Assessment" status="Completed">
                <Description>Evaluated all available datasets in the data directory to identify which datasets are ready for analysis versus those that need additional data collection.</Description>
                <ToolUsed>os.listdir, pathlib.Path.glob()</ToolUsed>
                <Input>/root/FCA/data/ directory structure</Input>
                <Output>Confirmed credit_card_fraud_2023 has CSV data ready for analysis; financial_phrasebank and ibm_aml directories exist but contain no CSV files</Output>
                <Rationale>Establish current dataset availability status to guide future data collection efforts and prioritize analysis on available datasets.</Rationale>
            </Step>
            <Step id="DP016" name="HuggingFace Credit Card Fraud Processing" status="Completed">
                <Description>Downloaded and processed HuggingFace CIS435-CreditCardFraudDetection dataset, removing unnamed columns and preparing for analysis.</Description>
                <ToolUsed>datasets.load_dataset, pandas.drop_duplicates(), column filtering</ToolUsed>
                <Input>HuggingFace dataset: dazzle-nu/CIS435-CreditCardFraudDetection (1,048,575 rows × 25 columns)</Input>
                <Output>Processed dataset: 1,048,575 rows × 23 columns, Class distribution: {0: 1,042,569, 1: 6,006}</Output>
                <Rationale>Acquire and clean real-world transaction data with significant class imbalance (99.4% normal, 0.6% fraud) for fraud detection model development.</Rationale>
            </Step>
            <Step id="DP017" name="Financial Phrasebank Dataset Processing" status="Completed">
                <Description>Downloaded and processed Financial Phrasebank dataset for sentiment analysis, handling encoding issues and processing both CSV and text formats.</Description>
                <ToolUsed>pandas.read_csv with latin1 encoding, text file parsing</ToolUsed>
                <Input>Financial Phrasebank CSV (4,845 rows) and text files (Sentences_*Agree.txt)</Input>
                <Output>Processed CSV: 4,839 rows, Processed sentences: 14,780 with sentiment labels (neutral: 8,951, positive: 3,988, negative: 1,841)</Output>
                <Rationale>Enable financial sentiment analysis capabilities by preparing labeled sentence data with varying agreement levels for NLP model training.</Rationale>
            </Step>
            <Step id="DP018" name="Dhanush Credit Card Fraud Processing" status="Completed">
                <Description>Downloaded and processed Dhanush Credit Card Fraud dataset from Kaggle, featuring behavioral and transaction-based fraud detection features.</Description>
                <ToolUsed>kaggle.api.dataset_download_files, pandas processing</ToolUsed>
                <Input>Kaggle dataset: dhanushnarayananr/credit-card-fraud (1,000,000 rows × 8 columns)</Input>
                <Output>Processed dataset: 1,000,000 rows × 8 columns, Fraud distribution: {0: 912,597, 1: 87,403} (8.7% fraud rate)</Output>
                <Rationale>Provide behavioral fraud detection features including distance metrics, purchase patterns, and payment methods for comprehensive fraud modeling.</Rationale>
            </Step>
            <Step id="DP019" name="WAMC Fraud Detection Processing" status="Completed">
                <Description>Downloaded and processed WAMC (whenamancodes) fraud detection dataset, featuring PCA-transformed anonymized transaction features.</Description>
                <ToolUsed>kaggle.api.dataset_download_files, duplicate removal</ToolUsed>
                <Input>Kaggle dataset: whenamancodes/fraud-detection (284,807 rows × 31 columns)</Input>
                <Output>Processed dataset: 283,726 rows × 31 columns after removing 1,081 duplicates, Class distribution: {0: 283,253, 1: 473} (0.17% fraud rate)</Output>
                <Rationale>Acquire classic anonymized fraud detection dataset with V1-V28 PCA features, Time, and Amount for benchmark fraud detection model comparison.</Rationale>
            </Step>
            <Step id="DP020" name="Customer Attrition Dataset Processing" status="Completed">
                <Description>Downloaded and processed Customer Attrition prediction dataset, focusing on credit card customer churn analysis with comprehensive customer metrics.</Description>
                <ToolUsed>kaggle.api.dataset_download_files, feature analysis</ToolUsed>
                <Input>Kaggle dataset: thedevastator/predicting-credit-card-customer-attrition-with-m (10,127 rows × 23 columns)</Input>
                <Output>Processed dataset: 10,127 rows × 23 columns, Attrition distribution: {'Existing Customer': 8,500, 'Attrited Customer': 1,627} (16.1% attrition rate)</Output>
                <Rationale>Enable customer retention analysis by providing comprehensive customer demographics, transaction behavior, and relationship data for churn prediction modeling.</Rationale>
            </Step>
            <Step id="DP021" name="Incribo Credit Card Fraud Processing" status="Completed">
                <Description>Downloaded and processed Incribo Credit Card Fraud dataset with comprehensive transaction features including merchant, location, and device information.</Description>
                <ToolUsed>kaggle.api.dataset_download_files, missing value handling</ToolUsed>
                <Input>Kaggle dataset: teamincribo/credit-card-fraud (8,000 rows × 20 columns)</Input>
                <Output>Processed dataset: 8,000 rows × 20 columns, Fraud distribution: {0: 4,011, 1: 3,989} (49.9% fraud rate), 6,053 missing values identified</Output>
                <Rationale>Provide detailed transaction-level fraud detection features including merchant information, geographic data, device fingerprinting, and payment method details for comprehensive fraud analysis.</Rationale>
            </Step>
            <Step id="DP022" name="IBM AML Dataset Assessment" status="Failed">
                <Description>Attempted to download IBM Transactions for Anti-Money Laundering dataset but encountered persistent download timeouts due to large file size.</Description>
                <ToolUsed>kaggle.api.dataset_download_files (multiple attempts with extended timeouts)</ToolUsed>
                <Input>Kaggle dataset: ealtman2019/ibm-transactions-for-anti-money-laundering-aml</Input>
                <Output>Download failed due to timeout errors after multiple attempts (120s, 300s timeouts tried)</Output>
                <Rationale>Identify infrastructure limitations for large dataset downloads and document the need for alternative download strategies or higher bandwidth solutions for future AML dataset acquisition.</Rationale>
            </Step>
        </Phase>
    </DataPreprocessing>

    <DataAnalysis>
        <Phase name="Data Analysis" description="Applying statistical and analytical methods to extract insights.">
            <Step id="DA001">
                <Description>Planned comprehensive data analysis strategy covering fraud detection, sentiment analysis, and customer attrition prediction with specific performance targets and technical approach.</Description>
                <ToolUsed>Strategic planning and documentation (data_analysis_strategy.md)</ToolUsed>
                <Input>7 processed datasets across 3 analytical domains</Input>
                <Output>Comprehensive analysis strategy document with phase-by-phase implementation plan</Output>
                <Rationale>Establish clear roadmap for systematic analysis covering EDA, modeling, and evaluation phases with defined success metrics and technical stack requirements.</Rationale>
            </Step>
            
            <Step id="DA002">
                <Description>Conducted comprehensive Exploratory Data Analysis (EDA) across all processed datasets including statistical summaries, distribution analysis, and correlation studies.</Description>
                <ToolUsed>exploratory_data_analysis.py using pandas, matplotlib, seaborn, plotly</ToolUsed>
                <Input>7 processed datasets with various sizes and characteristics</Input>
                <Output>Detailed EDA report with visualizations (fraud_class_distributions.png, correlation_matrix.png, sentiment_distribution.png, eda_report.json)</Output>
                <Rationale>Understand data characteristics, identify patterns, assess data quality, and inform subsequent modeling approaches through comprehensive statistical and visual analysis.</Rationale>
            </Step>
        </Phase>
    </DataAnalysis>

    <Modeling>
        <Phase name="Modeling" description="Building and evaluating predictive or descriptive models.">
            <Step id="MD001">
                <Description>Implemented fraud detection models using multiple machine learning algorithms across 5 fraud datasets with varying imbalance characteristics.</Description>
                <ToolUsed>quick_fraud_models.py using scikit-learn (LogisticRegression, RandomForestClassifier)</ToolUsed>
                <Input>5 fraud detection datasets: Credit Card Fraud 2023, HF Credit Card Fraud, Dhanush Fraud, WAMC Fraud, Incribo Fraud</Input>
                <Output>Fraud detection model comparison (quick_model_results.csv) with AUC-ROC scores ranging from 0.502 to 1.000</Output>
                <Rationale>Develop robust fraud detection capabilities across diverse data types and imbalance scenarios to support real-time fraud prevention systems.</Rationale>
            </Step>
            
            <Step id="MD002">
                <Description>Built sentiment analysis models for financial news text classification using natural language processing techniques.</Description>
                <ToolUsed>sentiment_analysis_model.py using TfidfVectorizer, MultinomialNB, LogisticRegression, RandomForestClassifier</ToolUsed>
                <Input>Financial Phrasebank dataset (14,780 sentences with neutral/positive/negative sentiment labels)</Input>
                <Output>Sentiment analysis results with Random Forest achieving 96.9% accuracy (sentiment_analysis_results.png, sentiment_model_results.csv)</Output>
                <Rationale>Enable automated financial news sentiment monitoring for market analysis and decision support with high accuracy classification of market sentiment.</Rationale>
            </Step>
            
            <Step id="MD003">
                <Description>Developed customer attrition prediction models to identify customers at risk of churning for proactive retention strategies.</Description>
                <ToolUsed>customer_attrition_model.py using LogisticRegression, RandomForestClassifier, GradientBoostingClassifier</ToolUsed>
                <Input>Customer Attrition dataset (10,127 customer records with 23 features)</Input>
                <Output>Perfect attrition prediction models achieving 1.000 AUC-ROC across all algorithms (customer_attrition_results.png, customer_attrition_model_results.csv)</Output>
                <Rationale>Implement predictive customer retention system enabling early identification of at-risk customers for targeted intervention campaigns.</Rationale>
            </Step>
            
            <Step id="MD004">
                <Description>Created comprehensive model comparison dashboard integrating results from all analysis domains for executive reporting and decision making.</Description>
                <ToolUsed>simple_dashboard.py using matplotlib for visualization and statistical summarization</ToolUsed>
                <Input>Model results from fraud detection, sentiment analysis, and customer attrition analyses</Input>
                <Output>Executive dashboard (simple_model_dashboard.png) and project summary report (project_summary.md) showing 100% success rate across all models</Output>
                <Rationale>Provide unified view of model performance across all domains to support strategic decision making and demonstrate project success with clear business value proposition.</Rationale>
            </Step>
        </Phase>
    </Modeling>

    <Deployment>
        <Phase name="Deployment" description="Deploying the solution, e.g., web application with portfolio.">
            <!-- Placeholder for future deployment steps -->
        </Phase>
    </Deployment>
</ProjectWorkflow>