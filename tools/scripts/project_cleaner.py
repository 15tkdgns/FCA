#!/usr/bin/env python3
"""
FCA í”„ë¡œì íŠ¸ ì •ë¦¬ ë„êµ¬
====================
ë¶ˆí•„ìš”í•œ íŒŒì¼ê³¼ í´ë”ë¥¼ ì‹ë³„í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤.
"""

import os
import shutil
import json
from pathlib import Path
from collections import defaultdict
import re
from datetime import datetime

class FCAProjectCleaner:
    def __init__(self, project_root="/root/FCA"):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "archive" / f"cleanup_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.cleanup_report = {
            'start_time': datetime.now().isoformat(),
            'files_deleted': [],
            'directories_deleted': [],
            'files_moved': [],
            'space_saved_mb': 0,
            'summary': {}
        }
        
        # ì •ë¦¬ ëŒ€ìƒ íŒ¨í„´ë“¤
        self.cleanup_patterns = {
            'cache_files': [
                '**/__pycache__',
                '**/*.pyc',
                '**/*.pyo',
                '**/*.pyd',
                '**/.pytest_cache',
                '**/htmlcov',
                '**/.coverage',
                '**/coverage.xml'
            ],
            'system_files': [
                '**/.DS_Store',
                '**/Thumbs.db',
                '**/*.tmp',
                '**/*.temp',
                '**/*~',
                '**/*.swp',
                '**/*.swo'
            ],
            'log_files': [
                '**/*.log.*',  # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼
                '**/logs/*.log.20*',  # ì˜¤ë˜ëœ ë¡œê·¸
            ],
            'duplicate_files': [],  # ëŸ°íƒ€ì„ì— ê°ì§€
            'empty_directories': [],  # ëŸ°íƒ€ì„ì— ê°ì§€
            'large_unnecessary_files': []  # ëŸ°íƒ€ì„ì— ê°ì§€
        }
        
        # ë³´ì¡´í•´ì•¼ í•  ì¤‘ìš” íŒŒì¼/í´ë”
        self.preserve_patterns = [
            'fca/**',
            'static_dashboard/**',
            'data/*.json',
            'requirements.txt',
            'README*.md',
            '*.py',
            'config/**',
            'docs/**'
        ]
    
    def analyze_project_structure(self):
        """í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        print("ğŸ“Š í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        
        structure_analysis = {
            'total_files': 0,
            'total_size_mb': 0,
            'file_types': defaultdict(int),
            'large_files': [],
            'directory_sizes': {},
            'duplicate_candidates': defaultdict(list)
        }
        
        # íŒŒì¼ ë¶„ì„
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    structure_analysis['total_files'] += 1
                    structure_analysis['total_size_mb'] += size_mb
                    
                    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
                    suffix = file_path.suffix.lower()
                    structure_analysis['file_types'][suffix or 'no_extension'] += 1
                    
                    # ëŒ€ìš©ëŸ‰ íŒŒì¼ (50MB ì´ìƒ)
                    if size_mb > 50:
                        structure_analysis['large_files'].append({
                            'path': str(file_path.relative_to(self.project_root)),
                            'size_mb': round(size_mb, 2)
                        })
                    
                    # ì¤‘ë³µ íŒŒì¼ í›„ë³´ (íŒŒì¼ëª… ê¸°ì¤€)
                    filename = file_path.name.lower()
                    structure_analysis['duplicate_candidates'][filename].append(str(file_path))
                    
                except (OSError, PermissionError):
                    continue
        
        # ë””ë ‰í† ë¦¬ í¬ê¸° ë¶„ì„
        for dir_path in self.project_root.rglob('*'):
            if dir_path.is_dir():
                try:
                    dir_size = sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file()) / (1024 * 1024)
                    if dir_size > 10:  # 10MB ì´ìƒë§Œ ê¸°ë¡
                        rel_path = str(dir_path.relative_to(self.project_root))
                        structure_analysis['directory_sizes'][rel_path] = round(dir_size, 2)
                except (OSError, PermissionError):
                    continue
        
        # ì‹¤ì œ ì¤‘ë³µ íŒŒì¼ í•„í„°ë§
        actual_duplicates = {k: v for k, v in structure_analysis['duplicate_candidates'].items() 
                           if len(v) > 1}
        structure_analysis['duplicate_candidates'] = actual_duplicates
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {structure_analysis['total_files']}ê°œ íŒŒì¼, {structure_analysis['total_size_mb']:.1f}MB")
        return structure_analysis
    
    def identify_cleanup_targets(self):
        """ì •ë¦¬ ëŒ€ìƒ íŒŒì¼ë“¤ ì‹ë³„"""
        print("ğŸ¯ ì •ë¦¬ ëŒ€ìƒ ì‹ë³„ ì¤‘...")
        
        cleanup_targets = {
            'safe_to_delete': [],
            'review_needed': [],
            'large_files': [],
            'duplicates': [],
            'empty_dirs': []
        }
        
        # 1. ìºì‹œ ë° ì„ì‹œ íŒŒì¼
        for category, patterns in self.cleanup_patterns.items():
            if category in ['cache_files', 'system_files']:
                for pattern in patterns:
                    for file_path in self.project_root.glob(pattern):
                        if file_path.exists():
                            size_mb = 0
                            if file_path.is_file():
                                size_mb = file_path.stat().st_size / (1024 * 1024)
                            elif file_path.is_dir():
                                size_mb = sum(f.stat().st_size for f in file_path.rglob('*') if f.is_file()) / (1024 * 1024)
                            
                            cleanup_targets['safe_to_delete'].append({
                                'path': str(file_path.relative_to(self.project_root)),
                                'type': category,
                                'size_mb': round(size_mb, 3)
                            })
        
        # 2. ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ (7ì¼ ì´ìƒ)
        logs_dir = self.project_root / 'logs'
        if logs_dir.exists():
            for log_file in logs_dir.glob('*.log.*'):
                try:
                    mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                    age_days = (datetime.now() - mtime).days
                    if age_days > 7:
                        size_mb = log_file.stat().st_size / (1024 * 1024)
                        cleanup_targets['safe_to_delete'].append({
                            'path': str(log_file.relative_to(self.project_root)),
                            'type': 'old_log',
                            'age_days': age_days,
                            'size_mb': round(size_mb, 3)
                        })
                except (OSError, PermissionError):
                    continue
        
        # 3. ë¹ˆ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        for dir_path in self.project_root.rglob('*'):
            if dir_path.is_dir() and dir_path != self.project_root:
                try:
                    if not any(dir_path.iterdir()):
                        cleanup_targets['empty_dirs'].append(str(dir_path.relative_to(self.project_root)))
                except (OSError, PermissionError):
                    continue
        
        # 4. ì¤‘ë³µ íŒŒì¼ ì°¾ê¸° (ë” ì •í™•í•œ ë°©ë²•)
        file_hashes = defaultdict(list)
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file() and file_path.stat().st_size > 1024:  # 1KB ì´ìƒ
                try:
                    # íŒŒì¼ í¬ê¸°ì™€ ì²˜ìŒ 1024ë°”ì´íŠ¸ë¡œ ê°„ë‹¨í•œ í•´ì‹œ
                    with open(file_path, 'rb') as f:
                        first_chunk = f.read(1024)
                    
                    file_key = (file_path.stat().st_size, hash(first_chunk))
                    file_hashes[file_key].append(file_path)
                except (OSError, PermissionError):
                    continue
        
        for file_list in file_hashes.values():
            if len(file_list) > 1:
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ì¤‘ë³µìœ¼ë¡œ í‘œì‹œ
                sorted_files = sorted(file_list, key=lambda x: x.stat().st_mtime, reverse=True)
                for dup_file in sorted_files[1:]:
                    size_mb = dup_file.stat().st_size / (1024 * 1024)
                    cleanup_targets['duplicates'].append({
                        'path': str(dup_file.relative_to(self.project_root)),
                        'original': str(sorted_files[0].relative_to(self.project_root)),
                        'size_mb': round(size_mb, 3)
                    })
        
        # 5. ê²€í†  í•„ìš”í•œ ëŒ€ìš©ëŸ‰ íŒŒì¼
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    if size_mb > 100:  # 100MB ì´ìƒ
                        cleanup_targets['large_files'].append({
                            'path': str(file_path.relative_to(self.project_root)),
                            'size_mb': round(size_mb, 2),
                            'type': file_path.suffix
                        })
                except (OSError, PermissionError):
                    continue
        
        return cleanup_targets
    
    def create_selective_backup(self, targets):
        """ì„ íƒì  ë°±ì—… ìƒì„±"""
        print(f"ğŸ’¾ ì¤‘ìš” íŒŒì¼ ë°±ì—… ìƒì„±: {self.backup_dir}")
        
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        backed_up_count = 0
        
        # ë³´ì¡´ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ë“¤ë§Œ ë°±ì—…
        for pattern in self.preserve_patterns:
            for file_path in self.project_root.glob(pattern):
                if file_path.is_file():
                    try:
                        rel_path = file_path.relative_to(self.project_root)
                        backup_path = self.backup_dir / rel_path
                        backup_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, backup_path)
                        backed_up_count += 1
                    except (OSError, PermissionError, shutil.Error):
                        continue
        
        print(f"âœ… ë°±ì—… ì™„ë£Œ: {backed_up_count}ê°œ íŒŒì¼")
        return backed_up_count
    
    def execute_cleanup(self, targets, auto_confirm=False):
        """ì •ë¦¬ ì‹¤í–‰"""
        if not auto_confirm:
            print("\nğŸ—‘ï¸ ì •ë¦¬ ëŒ€ìƒ ìš”ì•½:")
            print(f"  - ì•ˆì „ ì‚­ì œ: {len(targets['safe_to_delete'])}ê°œ")
            print(f"  - ì¤‘ë³µ íŒŒì¼: {len(targets['duplicates'])}ê°œ")
            print(f"  - ë¹ˆ ë””ë ‰í† ë¦¬: {len(targets['empty_dirs'])}ê°œ")
            print(f"  - ëŒ€ìš©ëŸ‰ íŒŒì¼: {len(targets['large_files'])}ê°œ (ê²€í†  í•„ìš”)")
            
            confirm = input("\nì •ë¦¬ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if confirm.lower() != 'y':
                print("âŒ ì •ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
        
        total_saved_mb = 0
        
        # 1. ì•ˆì „í•œ íŒŒì¼ë“¤ ì‚­ì œ
        print("ğŸ§¹ ìºì‹œ ë° ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        for item in targets['safe_to_delete']:
            file_path = self.project_root / item['path']
            try:
                if file_path.exists():
                    if file_path.is_file():
                        file_path.unlink()
                        self.cleanup_report['files_deleted'].append(item['path'])
                    elif file_path.is_dir():
                        shutil.rmtree(file_path)
                        self.cleanup_report['directories_deleted'].append(item['path'])
                    
                    total_saved_mb += item['size_mb']
            except (OSError, PermissionError) as e:
                print(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨: {item['path']} - {e}")
        
        # 2. ì¤‘ë³µ íŒŒì¼ ì‚­ì œ
        print("ğŸ“‹ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        for item in targets['duplicates']:
            file_path = self.project_root / item['path']
            try:
                if file_path.exists():
                    file_path.unlink()
                    self.cleanup_report['files_deleted'].append(item['path'])
                    total_saved_mb += item['size_mb']
            except (OSError, PermissionError) as e:
                print(f"âš ï¸ ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {item['path']} - {e}")
        
        # 3. ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ
        print("ğŸ“ ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘...")
        for dir_name in targets['empty_dirs']:
            dir_path = self.project_root / dir_name
            try:
                if dir_path.exists() and dir_path.is_dir():
                    dir_path.rmdir()
                    self.cleanup_report['directories_deleted'].append(dir_name)
            except (OSError, PermissionError) as e:
                print(f"âš ï¸ ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {dir_name} - {e}")
        
        self.cleanup_report['space_saved_mb'] = round(total_saved_mb, 2)
        print(f"âœ… ì •ë¦¬ ì™„ë£Œ: {total_saved_mb:.2f}MB ì ˆì•½")
        
        return True
    
    def reorganize_project_structure(self):
        """í”„ë¡œì íŠ¸ êµ¬ì¡° ì¬ì •ë¦¬"""
        print("ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ì¬ì •ë¦¬ ì¤‘...")
        
        # ì¤‘ë³µëœ ë°±ì—… ë””ë ‰í† ë¦¬ë“¤ ì •ë¦¬
        archive_dirs = list(self.project_root.glob('archive/frontend_backup_*'))
        if len(archive_dirs) > 3:  # ìµœì‹  3ê°œë§Œ ìœ ì§€
            archive_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            for old_backup in archive_dirs[3:]:
                try:
                    shutil.rmtree(old_backup)
                    print(f"  âœ… ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup.name}")
                except Exception as e:
                    print(f"  âš ï¸ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {old_backup.name} - {e}")
        
        # ë£¨íŠ¸ ë ˆë²¨ì˜ ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
        root_temp_files = [
            '*.png', '*.jpg', '*.jpeg',  # ì„ì‹œ ì´ë¯¸ì§€
            'test_*.py', '*_test.py',    # í…ŒìŠ¤íŠ¸ íŒŒì¼
            '*.tmp', '*.temp'            # ì„ì‹œ íŒŒì¼
        ]
        
        moved_count = 0
        for pattern in root_temp_files:
            for file_path in self.project_root.glob(pattern):
                if file_path.is_file():
                    # tools ë””ë ‰í† ë¦¬ë¡œ ì´ë™ ë˜ëŠ” ì‚­ì œ
                    if 'test' in file_path.name:
                        tests_dir = self.project_root / 'tests'
                        tests_dir.mkdir(exist_ok=True)
                        try:
                            shutil.move(str(file_path), tests_dir / file_path.name)
                            moved_count += 1
                        except Exception:
                            continue
                    elif file_path.suffix in ['.png', '.jpg', '.jpeg']:
                        # ì´ë¯¸ì§€ íŒŒì¼ì€ ì ì ˆí•œ ìœ„ì¹˜ë¡œ ì´ë™ ë˜ëŠ” ì‚­ì œ
                        if file_path.stat().st_size < 1024 * 1024:  # 1MB ë¯¸ë§Œë§Œ ì‚­ì œ
                            try:
                                file_path.unlink()
                                moved_count += 1
                            except Exception:
                                continue
        
        if moved_count > 0:
            print(f"  âœ… ë£¨íŠ¸ ë ˆë²¨ ì •ë¦¬: {moved_count}ê°œ íŒŒì¼ ì²˜ë¦¬")
        
        return moved_count
    
    def generate_cleanup_report(self):
        """ì •ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.cleanup_report['end_time'] = datetime.now().isoformat()
        self.cleanup_report['summary'] = {
            'total_files_deleted': len(self.cleanup_report['files_deleted']),
            'total_directories_deleted': len(self.cleanup_report['directories_deleted']),
            'total_space_saved_mb': self.cleanup_report['space_saved_mb']
        }
        
        report_file = self.project_root / 'cleanup_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.cleanup_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š ì •ë¦¬ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        return self.cleanup_report
    
    def run_full_cleanup(self, auto_confirm=False):
        """ì „ì²´ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ FCA í”„ë¡œì íŠ¸ ì •ë¦¬ ì‹œì‘")
        print("=" * 50)
        
        # 1. í”„ë¡œì íŠ¸ ë¶„ì„
        structure = self.analyze_project_structure()
        
        # 2. ì •ë¦¬ ëŒ€ìƒ ì‹ë³„
        targets = self.identify_cleanup_targets()
        
        # 3. ë°±ì—… ìƒì„±
        self.create_selective_backup(targets)
        
        # 4. ì •ë¦¬ ì‹¤í–‰
        success = self.execute_cleanup(targets, auto_confirm)
        
        if success:
            # 5. êµ¬ì¡° ì¬ì •ë¦¬
            self.reorganize_project_structure()
            
            # 6. ë¦¬í¬íŠ¸ ìƒì„±
            report = self.generate_cleanup_report()
            
            print("\nâœ… í”„ë¡œì íŠ¸ ì •ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ì ˆì•½ ê³µê°„: {report['space_saved_mb']}MB")
            print(f"ğŸ—‘ï¸ ì‚­ì œëœ íŒŒì¼: {len(report['files_deleted'])}ê°œ")
            print(f"ğŸ“ ì‚­ì œëœ ë””ë ‰í† ë¦¬: {len(report['directories_deleted'])}ê°œ")
            print(f"ğŸ’¾ ë°±ì—… ìœ„ì¹˜: {self.backup_dir}")
            
            return report
        
        return None

if __name__ == "__main__":
    import sys
    
    cleaner = FCAProjectCleaner()
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    auto_confirm = '--auto' in sys.argv or '-y' in sys.argv
    
    if '--analyze-only' in sys.argv:
        # ë¶„ì„ë§Œ ì‹¤í–‰
        structure = cleaner.analyze_project_structure()
        targets = cleaner.identify_cleanup_targets()
        
        print(f"\nğŸ“Š ì •ë¦¬ ëŒ€ìƒ ìš”ì•½:")
        print(f"  - ì•ˆì „ ì‚­ì œ: {len(targets['safe_to_delete'])}ê°œ")
        print(f"  - ì¤‘ë³µ íŒŒì¼: {len(targets['duplicates'])}ê°œ")
        print(f"  - ë¹ˆ ë””ë ‰í† ë¦¬: {len(targets['empty_dirs'])}ê°œ")
        print(f"  - ëŒ€ìš©ëŸ‰ íŒŒì¼: {len(targets['large_files'])}ê°œ")
        
        # ì˜ˆìƒ ì ˆì•½ ê³µê°„
        total_savings = sum(item['size_mb'] for item in targets['safe_to_delete'])
        total_savings += sum(item['size_mb'] for item in targets['duplicates'])
        print(f"  - ì˜ˆìƒ ì ˆì•½: {total_savings:.2f}MB")
        
    else:
        # ì „ì²´ ì •ë¦¬ ì‹¤í–‰
        cleaner.run_full_cleanup(auto_confirm)