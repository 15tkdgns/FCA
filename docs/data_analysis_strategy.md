# 데이터 분석 및 모델링 전략

## 🎯 분석 목표

### 1. 사기 탐지 모델링 (주요 목표)
- **다양한 불균형 데이터셋**을 활용한 종합적 사기 탐지 시스템 구축
- **균형 vs 불균형** 데이터에서의 모델 성능 비교
- **행동 패턴 기반** 사기 탐지 알고리즘 개발

### 2. 감정 분석 시스템
- **금융 뉴스 감정 분석** 모델 구축
- **실시간 감정 예측** 시스템 개발

### 3. 고객 이탈 예측
- **고객 유지 전략** 지원 모델
- **이탈 위험 고객** 조기 식별

## 📊 데이터셋 활용 계획

### 사기 탐지용 데이터셋 (5개)
| 데이터셋 | 크기 | 불균형 정도 | 특징 | 활용 목적 |
|----------|------|-------------|------|-----------|
| **CC Fraud 2023** | 568K | 완벽 균형 (50/50) | PCA 피처 | 균형 데이터 모델 |
| **HF CC Fraud** | 1.05M | 극도 불균형 (0.6%) | 실제 거래 정보 | 실제 환경 모델 |
| **Dhanush Fraud** | 1M | 중간 불균형 (8.7%) | 행동 패턴 | 행동 기반 모델 |
| **WAMC Fraud** | 284K | 극도 불균형 (0.17%) | PCA 피처 | 벤치마크 모델 |
| **Incribo Fraud** | 8K | 완벽 균형 (49.9%) | 상세 거래 | 상세 분석 모델 |

### 기타 분석용 데이터셋 (2개)
- **Financial Phrasebank** (14.8K): 감정 분석
- **Customer Attrition** (10.1K): 이탈 예측

## 🔍 분석 단계별 계획

### Phase 1: 탐색적 데이터 분석 (EDA)
#### 1.1 기본 통계 분석
- 각 데이터셋의 분포 특성 파악
- 클래스 불균형 시각화
- 피처별 분포 및 상관관계 분석

#### 1.2 사기 패턴 분석
- 정상 vs 사기 거래의 특징 비교
- 시간대별, 금액대별 사기 패턴
- 지리적 분포 분석 (가능한 경우)

#### 1.3 데이터 품질 분석
- 결측값 패턴 분석
- 이상치(Outlier) 탐지
- 피처 중요도 예비 분석

### Phase 2: 데이터 전처리 및 피처 엔지니어링
#### 2.1 고급 전처리
- 스케일링 및 정규화
- 범주형 변수 인코딩
- 시계열 피처 생성 (가능한 경우)

#### 2.2 피처 엔지니어링
- 파생 변수 생성
- 피처 선택 및 차원 축소
- 도메인 지식 기반 피처 생성

#### 2.3 불균형 데이터 처리
- SMOTE, ADASYN 등 오버샘플링
- 언더샘플링 기법
- 클래스 가중치 조정

### Phase 3: 모델 개발 및 평가
#### 3.1 베이스라인 모델
- Logistic Regression
- Random Forest
- Gradient Boosting (XGBoost)

#### 3.2 고급 모델
- LightGBM, CatBoost
- Neural Networks
- Ensemble Methods

#### 3.3 모델 평가 지표
- **불균형 데이터 특화 지표**:
  - Precision, Recall, F1-Score
  - AUC-ROC, AUC-PR
  - Matthews Correlation Coefficient (MCC)
- **비즈니스 지표**:
  - False Positive Rate (고객 불편)
  - False Negative Rate (손실)
  - Cost-sensitive evaluation

### Phase 4: 모델 최적화 및 해석
#### 4.1 하이퍼파라미터 튜닝
- Grid Search, Random Search
- Bayesian Optimization
- Cross-validation 전략

#### 4.2 모델 해석성
- SHAP (SHapley Additive exPlanations)
- LIME (Local Interpretable Model-Agnostic Explanations)
- 피처 중요도 분석

#### 4.3 모델 앙상블
- Voting Classifier
- Stacking
- Blending

## 🛠️ 기술 스택

### 데이터 분석
- **pandas, numpy**: 데이터 조작
- **matplotlib, seaborn, plotly**: 시각화
- **scipy**: 통계 분석

### 머신러닝
- **scikit-learn**: 기본 ML 알고리즘
- **xgboost, lightgbm, catboost**: 고급 부스팅
- **imbalanced-learn**: 불균형 데이터 처리
- **optuna**: 하이퍼파라미터 최적화

### 딥러닝 (필요시)
- **tensorflow/keras**: 신경망
- **pytorch**: 고급 딥러닝

### 모델 해석성
- **shap**: 모델 해석
- **lime**: 지역적 해석

### 웹 대시보드
- **plotly-dash**: 인터랙티브 대시보드
- **streamlit**: 빠른 프로토타이핑

## 📈 성과 지표 및 목표

### 사기 탐지 모델 목표
- **Precision**: > 0.8 (오탐 최소화)
- **Recall**: > 0.9 (놓치는 사기 최소화)
- **F1-Score**: > 0.85
- **AUC-ROC**: > 0.95

### 감정 분석 모델 목표
- **Accuracy**: > 0.85
- **Macro F1-Score**: > 0.8

### 고객 이탈 예측 목표
- **AUC-ROC**: > 0.85
- **Precision@10%**: > 0.6 (상위 10% 예측 정확도)

## 🚀 구현 순서

### Week 1: EDA 및 데이터 이해
1. **Day 1-2**: 기본 EDA 및 시각화
2. **Day 3-4**: 사기 패턴 심화 분석
3. **Day 5**: 데이터 품질 평가 및 정리

### Week 2: 피처 엔지니어링 및 전처리
1. **Day 1-2**: 고급 전처리 파이프라인
2. **Day 3-4**: 피처 엔지니어링
3. **Day 5**: 불균형 데이터 처리 전략

### Week 3: 모델 개발
1. **Day 1-2**: 베이스라인 모델 구축
2. **Day 3-4**: 고급 모델 개발
3. **Day 5**: 모델 평가 및 비교

### Week 4: 최적화 및 배포
1. **Day 1-2**: 하이퍼파라미터 튜닝
2. **Day 3-4**: 모델 해석 및 앙상블
3. **Day 5**: 대시보드 구축 및 배포

## 📋 리스크 및 대응 방안

### 기술적 리스크
- **메모리 부족**: 배치 처리, 샘플링 활용
- **계산 시간**: 병렬 처리, 모델 단순화
- **과적합**: 교차 검증, 정규화 강화

### 데이터 리스크
- **데이터 편향**: 다양한 데이터셋 활용
- **레이블 노이즈**: 앙상블로 robust 모델 구축
- **개념 변화**: 시간 기반 검증 전략

## 📚 참고 자료 및 벤치마크
- **학술 논문**: 최신 사기 탐지 알고리즘
- **Kaggle 대회**: 유사 문제 해결 방법
- **오픈소스**: 검증된 구현체 활용

---

*전략 수립 완료: 2025-07-25*  
*다음 단계: 탐색적 데이터 분석 시작* 🚀